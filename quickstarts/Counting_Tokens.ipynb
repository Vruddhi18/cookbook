{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7IU65cnKnWM"
      },
      "source": [
        "##### Copyright 2024 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Y_3Mok6dKq-j"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZXn1Salxl_w"
      },
      "source": [
        "# Gemini API: All about tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FIB-JDtxgUE"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/gemini-api-cookbook/blob/main/quickstarts/Token_Counting.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRzxdrjKLTJa"
      },
      "source": [
        "An understanding of tokens is central to using the Gemini API. This guide will provide an interactive introduction to what a token is and how they are used in the Gemini API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAJjAy6S3iGF"
      },
      "source": [
        "## About tokens\n",
        "\n",
        "LLMs break up their input and produce their output at a granularity that is smaller than a word, but larger than a single character or code-point.\n",
        "\n",
        "The objective will vary from implementation to implementation, but often the vocabulary is generated to minimise the total number of tokens required across a corpus. This encourages high-frequency words (such as `the` in English) to be represented by fewer ***tokens***, while lower-frequency words will be comprised of more tokens.\n",
        "\n",
        "Tokens can be single characters, like `z`, or whole words, like `the`.\n",
        "\n",
        "The specific vocabulary used is learned ahead of time in a process called \"tokenization\". As the Gemini API's vocabulary is considered an implementation detail, this guide will not go into detail, but you can find a technical deep-dive in TensorFlow's [Sub-word Tokenization](https://www.tensorflow.org/text/guide/subwords_tokenizer) tutorial, including the section on [the algorithm](https://www.tensorflow.org/text/guide/subwords_tokenizer#optional_the_algorithm)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KfQuwN63Zg9"
      },
      "source": [
        "## Tokens in the Gemini API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HpS7dkL3e4S"
      },
      "source": [
        "### Set up the API\n",
        "\n",
        "You'll use the API to explore these concepts, so start by setting up the Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GBa_hMFneZKO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/137.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -U -q google-generativeai  # Install the Python SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OzsRfmWrxd_F"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG_wSwTJ2wAP"
      },
      "source": [
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](https://github.com/google-gemini/gemini-api-cookbook/blob/main/quickstarts/Authentication.ipynb) quickstart for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LyWgDDHr1yxd"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etlFvMXP3Gb7"
      },
      "source": [
        "### Context windows\n",
        "\n",
        "The models available through the Gemini API have context windows that are measured in tokens. These define how much input you can provide, and how much output the model can generate, and combined are referred to as the \"context window\". This information is available directly through [the API](https://ai.google.dev/api/rest/v1/models/get) and in the [models](https://ai.google.dev/models/gemini) documentation.\n",
        "\n",
        "In this example you can see the `gemini-1.0-pro-latest` model has an input of 30k tokens and an output of 2k tokens, giving a total context window of 32k tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1QC23D2z3GLV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(30720, 2048)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_info = genai.get_model('models/gemini-1.0-pro-latest')\n",
        "(model_info.input_token_limit, model_info.output_token_limit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkh8v5QI4v5h"
      },
      "source": [
        "## Counting tokens\n",
        "\n",
        "The API provides [an endpoint](https://ai.google.dev/api/rest/v1/models/embedContent) for counting the number of tokens in a request: [`GenerativeModel.count_tokens`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#count_tokens). You pass the same arguments as you would to the [`GenerativeModel.generate_content`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content) call and the service will return the number of tokens in that request."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0J8JPYbCGnv"
      },
      "source": [
        "### Text tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "7jpoJFpX5Cu_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "total_tokens: 10"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = genai.GenerativeModel('models/gemini-1.0-pro-latest')\n",
        "\n",
        "model.count_tokens(\"The quick brown fox jumps over the lazy dog.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQzJ7asV-HJB"
      },
      "source": [
        "### Multi-turn tokens\n",
        "\n",
        "Multi-turn conversational (chat) objects work similarly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "eqUpyE_E95_w"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "total_tokens: 8"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat = model.start_chat(history=['Hi my name is Bob',  'Hi Bob!'])\n",
        "\n",
        "model.count_tokens(chat.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMvjgRkVAvVN"
      },
      "source": [
        "To understand how big your next conversational turn will be, you will need to append it to the history when you call `count_tokens`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "pxVsykc5A5he"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "total_tokens: 15"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from google.generativeai.types.content_types import to_contents\n",
        "\n",
        "model.count_tokens(chat.history + to_contents('What is the meaning of life?'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZYcaUXl-Sna"
      },
      "source": [
        "### Multi-modal tokens\n",
        "\n",
        "All input to the API is tokenized, including images or other non-text modalities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Jzwrahub-ez5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "total_tokens: 263"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from PIL import Image\n",
        "\n",
        "!wget -q https://goo.gle/instrument-img -O instrument.jpg\n",
        "organ = Image.open('instrument.jpg')\n",
        "\n",
        "model.count_tokens(['Tell me about this instrument', organ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXF0vpdG_H_Q"
      },
      "source": [
        "Internally, images are a fixed size, so they consume a fixed number of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "hc0CBsl6_Tkk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2048, 1362)\n",
            "total_tokens: 258\n",
            "\n",
            "(768, 1024)\n",
            "total_tokens: 258\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -q https://goo.gle/sketch-img -O sketch.jpg\n",
        "sketch = Image.open('sketch.jpg')\n",
        "\n",
        "print(organ.size)\n",
        "print(model.count_tokens(organ))\n",
        "\n",
        "print(sketch.size)\n",
        "print(model.count_tokens(sketch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-IRQfQdJKfp"
      },
      "source": [
        "## Further reading\n",
        "\n",
        "For more on token counting, check out these articles.\n",
        "\n",
        "* [`countTokens`](https://ai.google.dev/api/rest/v1/models/countTokens) REST API reference,\n",
        "* [`count_tokens`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#count_tokens) Python API reference,\n",
        "* TensorFlow Text's [Sub-word tokenization](https://www.tensorflow.org/text/guide/subwords_tokenizer) tutorial."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "v7IU65cnKnWM"
      ],
      "name": "Counting_Tokens.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
